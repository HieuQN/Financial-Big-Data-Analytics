{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "MATH 5671_Assignment002_Group5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen084/MATH5671/blob/master/MATH_5671_Assignment002_Group5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5prKZf1ZDYun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source: https://github.com/ChakChak1234/FrozenLake-Using-Reinforcement-and-Tensorflow/blob/master/OpenGymAI_FrozenLake.ipynb\n",
        "#The code below is modified and inspired by the source above.\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf #import tensorflow\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "env = gym.make('FrozenLake-v0')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxovVjiTSVXD",
        "colab_type": "code",
        "outputId": "eb6d107a-cfea-41dd-e5f0-e19550de0c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "#Setting the parametesr\n",
        "gamma = 0.8\n",
        "epsilon = 0.2\n",
        "episodes = 2000\n",
        "alpha = 0.1\n",
        "totalReward = 0\n",
        "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32) #There are total of 16 input represent 4x4 matrix\n",
        "W = tf.Variable(tf.random_uniform([16,4],0,0.1)) #Initialize weights \n",
        "Q1 = tf.matmul(inputs1,W) #Getting the initial Q value\n",
        "output = tf.argmax(Q1,1) #Making decision by choosing the max value\n",
        "\n",
        "Q2 = tf.placeholder(shape=[1,4],dtype=tf.float32) #Create a placeholder for the next Q\n",
        "loss = tf.reduce_sum(tf.square(Q2 - Q1)) #Loss function\n",
        "gdo = tf.train.GradientDescentOptimizer(learning_rate=alpha) #Gradient decent for the lost function\n",
        "updatedweights = gdo.minimize(loss) \n",
        "\n",
        "session = tf.Session() #Start the tensorflow session\n",
        "session.run(tf.initialize_all_variables())\n",
        "for i in range(episodes):\n",
        "    state_now = env.reset() #Reset to state 1\n",
        "    done = False\n",
        "    reward = 0 #Reset the reward\n",
        "    while not done: #Either reach the goal or stepped on ice\n",
        "        #Start training by feeding the input\n",
        "        action , Y = session.run([output, Q1], feed_dict = {inputs1 : [np.eye(16)[state_now]]})\n",
        "        if epsilon > np.random.rand(1):\n",
        "            action[0] = env.action_space.sample()\n",
        "            epsilon -= 10**-3\n",
        "        #Feed the action into the env\n",
        "        state_next , reward, done, info = env.step(action[0])\n",
        "        #Run the next state\n",
        "        Y1 = session.run(Q1, feed_dict = {inputs1 : [np.eye(16)[state_next]]})\n",
        "        change_Y = Y\n",
        "        change_Y[0, action[0]] = reward + gamma*np.max(Y1)\n",
        "        #Updating the weights \n",
        "        _,new_weights = session.run([updatedweights,W],feed_dict={inputs1:[np.eye(16)[state_now]],Q2:change_Y})\n",
        "        #Update the reward\n",
        "        totalReward += reward\n",
        "        state_now = state_next\n",
        "        state = state_next\n",
        "session.close()\n",
        "print(\"Average rewards: \" + str(totalReward/episodes))\n",
        "print(new_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average rewards: 0.047\n",
            "[[0.03153995 0.03146403 0.03692959 0.03059714]\n",
            " [0.03647947 0.05054735 0.03255894 0.03711748]\n",
            " [0.03927423 0.04309006 0.06531671 0.0411752 ]\n",
            " [0.06281664 0.00269843 0.03600845 0.01536837]\n",
            " [0.04205105 0.0689192  0.04235161 0.04051131]\n",
            " [0.00636323 0.09605737 0.06818032 0.00304327]\n",
            " [0.06844991 0.09083297 0.07142799 0.02214117]\n",
            " [0.09227116 0.02762512 0.07564083 0.07379955]\n",
            " [0.04856469 0.11083631 0.03478896 0.0475601 ]\n",
            " [0.08467879 0.03189382 0.14692073 0.07191397]\n",
            " [0.23463358 0.01314266 0.03373578 0.00739723]\n",
            " [0.05090854 0.05213565 0.0887234  0.0983282 ]\n",
            " [0.05368569 0.01357607 0.05510708 0.06938589]\n",
            " [0.07258905 0.02412957 0.02439753 0.26879483]\n",
            " [0.05704917 0.01130438 0.63143975 0.06611975]\n",
            " [0.08111747 0.0783421  0.05911888 0.0294739 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFv9-4pqt0nA",
        "colab_type": "code",
        "outputId": "8c31316c-f615-4ef5-88e8-ac7f7db473a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "env.reset()\n",
        "episodes = 20\n",
        "max_steps =1000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    clear_output()\n",
        "    print(\"*****************\")\n",
        "    print(\"Episodes\", episode)\n",
        "    print(\"*****************\")\n",
        "    \n",
        "    for step in range(max_steps):        \n",
        "        # Take the action that has the best Q value\n",
        "        action = np.argmax(new_weights[state])        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            print(\"*****************\")\n",
        "            print(\"Steps\", step)\n",
        "            print(\"*****************\")\n",
        "            time.sleep(2)\n",
        "            break\n",
        "        state = new_state\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****************\n",
            "Episodes 19\n",
            "*****************\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "*****************\n",
            "Steps 2\n",
            "*****************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
