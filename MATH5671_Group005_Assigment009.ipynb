{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MATH5671_Group005_Assigment009.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen084/MATH5671/blob/master/MATH5671_Group005_Assigment009.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqzHxppVQ-hV",
        "colab_type": "text"
      },
      "source": [
        "Drive mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMht7RcQLFy0",
        "colab_type": "code",
        "outputId": "db51a8b7-c480-403b-c03a-83965f74c825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfQDBR0WRGGz",
        "colab_type": "text"
      },
      "source": [
        "Something is wrong with Spark 2.4.4 and Java 8. Should use lower version 2.2.3. Also only Java 8 works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHwn4IpSNlyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-2.2.3/spark-2.2.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.2.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUSvIhXDO613",
        "colab_type": "code",
        "outputId": "fe89e3ab-bd17-4cf1-add9-9b0bd8da3c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data  spark-2.2.3-bin-hadoop2.7\tspark-2.2.3-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPSkxZu-RSWn",
        "colab_type": "text"
      },
      "source": [
        "Install Java 8 if Java 11 was installed. Spark is compatible with Java 8 now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gix1_VLcQc0f",
        "colab_type": "code",
        "outputId": "7dbbe0d3-ec66-44ba-bcae-ac8c534df9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_222\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10)\n",
            "OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2zSCDcLRlxI",
        "colab_type": "text"
      },
      "source": [
        "Switch to Java 8 if it is Java 11 now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeIEwLesQjuJ",
        "colab_type": "code",
        "outputId": "95bb69bb-35bd-4967-b037-fb0726af410e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!update-alternatives --config java"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "* 2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDM2kLSbR18O",
        "colab_type": "text"
      },
      "source": [
        "Double check if it is Java 8 now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3uLhoZZQ0pU",
        "colab_type": "code",
        "outputId": "647e1180-0dd1-4ed6-c59e-3e664bbfe8d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!java -version "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_222\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10)\n",
            "OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaxULuoMRtxd",
        "colab_type": "text"
      },
      "source": [
        "Set some environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uADy95nbQsCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.2.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHzvVnvrUVu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RArYnhRxUmZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "244f3997-5003-4876-8b4d-5d584f50a83a"
      },
      "source": [
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "conf = SparkConf().setAppName(\"MLlib\")\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.setLogLevel(\"WARN\")\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7b190903ed19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MLlib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.2.3-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-2.2.3-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    303\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 305\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MLlib, master=local[*]) created by __init__ at <ipython-input-8-7b190903ed19>:5 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7s18DRZ6eyB",
        "colab_type": "text"
      },
      "source": [
        "Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrCtqiIEP_4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathdir ='/content/gdrive/My Drive/Academic/UCONN/Financial Research/MATH5671/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmIHqtEc0AuZ",
        "colab_type": "code",
        "outputId": "1f0f11c2-a389-4b84-9887-142eb52dfc67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Load dataframe#\n",
        "payment = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'payment.csv').alias(\"payment\").createOrReplaceTempView(\"payment\")\n",
        "rental = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'rental.csv').alias('rental').createOrReplaceTempView(\"rental\")\n",
        "staff = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'staff.csv').alias('staff').createOrReplaceTempView(\"staff\")\n",
        "inventory = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'inventory.csv').alias('inventory').createOrReplaceTempView(\"inventory\")\n",
        "store = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'store.csv').alias('store').createOrReplaceTempView(\"store\")\n",
        "address = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'address.csv').alias('address').createOrReplaceTempView(\"address\")\n",
        "customer = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'customer.csv').alias('customer').createOrReplaceTempView(\"customer\")\n",
        "film = spark.read.format(\"csv\").option(\"header\", \"true\").load(pathdir+'film.csv').alias('film').createOrReplaceTempView(\"film\")\n",
        "\n",
        "print('Question 1: Payment DataFrame')\n",
        "spark.sql(\"select customer_id, sum(amount) as total_amount from payment group by customer_id order by total_amount DESC\").show(3)\n",
        "\n",
        "print('Question 2: Rental Dataframe')\n",
        "spark.sql(\"select * from rental where year(rental_date) = 2005 order by return_date\").show(3)\n",
        "\n",
        "print('Question 3: Count Staff ID')\n",
        "spark.sql(\"select staff_id, count(rental_id) as count from rental group by staff_id\").show()\n",
        "\n",
        "print('Question 4: Count by first and last name')\n",
        "spark.sql(\"select first(first_name) as first_name, first(last_name) as last_name, count(rental_id) as count from staff inner join rental on rental.staff_id = staff.staff_id group by staff.staff_id\").show()\n",
        "\n",
        "print('Question 5:')\n",
        "spark.sql(\"\"\"select rental.rental_date, rental.return_date, address.address, film.title, film.description, customer.first_name, customer.last_name, staff.first_name, staff.last_name from rental join inventory on rental.inventory_id = inventory.inventory_id join store on inventory.store_id = store.store_id join address on store.address_id = address.address_id join customer on rental.customer_id = customer.customer_id join staff on staff.store_id = customer.store_id join film on inventory.film_id = film.film_id order by rental.rental_date\"\"\").show(3)\n",
        "\n",
        "print('Question 6:')\n",
        "spark.sql(\"\"\"select customer_id, total_amount from (select customer_id, sum(amount) as total_amount from payment where year(payment_date) =2005 group by customer_id)\n",
        "where total_amount > 200\"\"\").show()\n",
        "\n",
        "print('Question 7:')\n",
        "spark.sql(\"\"\"select * from (select first(customer.first_name) as first_name, first(customer.last_name) as last_name, sum(amount) as total_amount from payment inner join customer on customer.customer_id = payment.customer_id where year(payment_date) =2005 group by payment.customer_id) where total_amount > 200 \"\"\").show()\n",
        "\n",
        "print('Question 8:')\n",
        "year2005 = spark.sql(\"select customer_id, year(rental_date) as rentdate from rental where year(rental_date) = 2005\").createOrReplaceTempView(\"year2005\")\n",
        "year2006 = spark.sql(\"select customer_id, year(rental_date) as rentdate from rental where year(rental_date) = 2006\").createOrReplaceTempView(\"year2006\")\n",
        "spark.sql(\"select customer_id, year(rental_date) as rentdate from rental where year(rental_date) = 2005\").show(3)\n",
        "spark.sql(\"select customer_id, year(rental_date) as rentdate from rental where year(rental_date) = 2006\").show(3)\n",
        "spark.sql(\"\"\"select count(distinct(year2005.customer_id)) as customer_id_in_2005_not_2006 from year2005 where year2005.customer_id not in (select year2006.customer_id from year2006) \"\"\").show()\n",
        "spark.sql(\"\"\"select count(year2006.customer_id) as customer_id_in_2006_not_2005 from year2006 where year2006.customer_id not in ( select year2005.customer_id from year2005) \"\"\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question 1: Payment DataFrame\n",
            "+-----------+------------------+\n",
            "|customer_id|      total_amount|\n",
            "+-----------+------------------+\n",
            "|        526| 221.5500000000001|\n",
            "|        148| 216.5400000000001|\n",
            "|        144|195.58000000000007|\n",
            "+-----------+------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Question 2: Rental Dataframe\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
            "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
            "|       32|2005-05-25 04:06:21|        3832|        230|2005-05-25 23:55:21|       1|2006-02-15 21:30:53|\n",
            "|       21|2005-05-25 01:59:46|         146|        388|2005-05-26 01:01:46|       2|2006-02-15 21:30:53|\n",
            "|       14|2005-05-25 00:31:15|        2701|        446|2005-05-26 02:56:15|       1|2006-02-15 21:30:53|\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Question 3: Count Staff ID\n",
            "+--------+-----+\n",
            "|staff_id|count|\n",
            "+--------+-----+\n",
            "|       1| 8040|\n",
            "|       2| 8004|\n",
            "+--------+-----+\n",
            "\n",
            "Question 4: Count by first and last name\n",
            "+----------+---------+-----+\n",
            "|first_name|last_name|count|\n",
            "+----------+---------+-----+\n",
            "|      Mike|  Hillyer| 8040|\n",
            "|       Jon| Stephens| 8004|\n",
            "+----------+---------+-----+\n",
            "\n",
            "Question 5:\n",
            "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
            "|        rental_date|        return_date|           address|          title|         description|first_name|last_name|first_name|last_name|\n",
            "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
            "|2005-05-24 22:53:30|2005-05-26 22:04:30| 47 MySakila Drive|BLANKET BEVERLY|A Emotional Docum...| CHARLOTTE|   HUNTER|      Mike|  Hillyer|\n",
            "|2005-05-24 22:54:33|2005-05-28 19:40:33|28 MySQL Boulevard|   FREAKY POCUS|A Fast-Paced Docu...|     TOMMY|  COLLAZO|      Mike|  Hillyer|\n",
            "|2005-05-24 23:03:39|2005-06-01 22:12:39|28 MySQL Boulevard|  GRADUATE LORD|A Lacklusture Epi...|    MANUEL|  MURRELL|      Mike|  Hillyer|\n",
            "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Question 6:\n",
            "+-----------+-----------------+\n",
            "|customer_id|     total_amount|\n",
            "+-----------+-----------------+\n",
            "|        526|221.5500000000001|\n",
            "|        148|216.5400000000001|\n",
            "+-----------+-----------------+\n",
            "\n",
            "Question 7:\n",
            "+----------+---------+-----------------+\n",
            "|first_name|last_name|     total_amount|\n",
            "+----------+---------+-----------------+\n",
            "|      KARL|     SEAL|221.5500000000001|\n",
            "|   ELEANOR|     HUNT|216.5400000000001|\n",
            "+----------+---------+-----------------+\n",
            "\n",
            "Question 8:\n",
            "+-----------+--------+\n",
            "|customer_id|rentdate|\n",
            "+-----------+--------+\n",
            "|        130|    2005|\n",
            "|        459|    2005|\n",
            "|        408|    2005|\n",
            "+-----------+--------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----------+--------+\n",
            "|customer_id|rentdate|\n",
            "+-----------+--------+\n",
            "|        155|    2006|\n",
            "|        335|    2006|\n",
            "|         83|    2006|\n",
            "+-----------+--------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+----------------------------+\n",
            "|customer_id_in_2005_not_2006|\n",
            "+----------------------------+\n",
            "|                         441|\n",
            "+----------------------------+\n",
            "\n",
            "+----------------------------+\n",
            "|customer_id_in_2006_not_2005|\n",
            "+----------------------------+\n",
            "|                           0|\n",
            "+----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}